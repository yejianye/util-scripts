#!/usr/bin/env python
import yaml
import os
import sys
import re
import time
from argh import arg, dispatch_command
from threading import Thread
from Queue import Queue
import requests

class LoggerThread(Thread):
    def __init__(self, job_count, complete_queue, filename):
        super(LoggerThread, self).__init__()
        self.complete_queue = complete_queue
        self.file = open(filename, 'a')
        self.job_count = job_count
        self.completed_count = 0
        self.daemon = True

    def run(self):
        while self.completed_count < self.job_count:
            url = self.complete_queue.get()
            print 'Finished %s (%d/%d)' % (url, self.completed_count, self.job_count)
            self.file.write(url + '\n')
            self.completed_count += 1
        self.file.close()

    def shutdown(self):
        self.file.close()

class DownloadThread(Thread):
    def __init__(self, work_queue, complete_queue, delay, cookies):
        super(DownloadThread, self).__init__()
        self.work_queue = work_queue
        self.complete_queue = complete_queue
        self.delay = delay
        if cookies:
            self.cookies = yaml.load(open(cookies)) 
            self.cookies = dict((k, str(v)) for k, v in self.cookies.iteritems())
        else:
            self.cookies = None

    def run(self):
        while not self.work_queue.empty():
            url = self.work_queue.get()
            print 'Start downloading', url
            resp = requests.get(url, cookies=self.cookies)
            filename = None
            content_disposition = resp.headers.get('content-disposition', '')
            if 'filename' in content_disposition:
                match = re.search('filename=([^ ]+)', content_disposition)
                if match:
                    filename = match.groups()[0]
            if not filename:
                filename = resp.request.path_url[1:] + '.html'
            f = open(filename, 'w')
            f.write(resp.content)
            f.close()
            self.complete_queue.put(url)
            time.sleep(self.delay)

def download(urls, thread_count, delay, cookies=None):
    log_filename = 'downloader.log'
    if os.path.exists(log_filename):
        completed_urls = set([x.strip() for x in open(log_filename).readlines()])
        urls = [url for url in urls if url not in completed_urls]
    work_queue = Queue()
    map(work_queue.put, urls)
    complete_queue= Queue()
    workers = [DownloadThread(work_queue, complete_queue, delay, cookies) for i in xrange(thread_count)]
    logger = LoggerThread(len(urls), complete_queue, log_filename)
    threads = workers + [logger]
    [t.start() for t in threads]
    try:
        [t.join() for t in threads]
    except KeyboardInterrupt:
        logger.shutdown()
        [t.kill() for t in threads]

@arg('--parallel', '-p', default=5, help='Max number of parallel downloads. (default 5)')
@arg('--delay', '-d', default=0, help='Number of seconds delay between 2 file downloads in one thread. (default 0)')
@arg('--cookies', '-c', help='Load cookies from file')
@arg('--file', '-f', help='File that contains urls for downloading. If not specified, urls will be fetched from stdin.')
def main(args):
    input = open(args.file) if args.file else sys.stdin 
    urls = [x.strip() for x in input.readlines()]
    download(urls, args.parallel, args.delay, args.cookies)


if __name__ == '__main__':
    dispatch_command(main)
